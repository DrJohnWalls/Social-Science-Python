{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Class 11 & 12 - Grabbing Tweets with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Configure regular Twitter API\n",
    "# Note that this cell has been separated\n",
    "# To accesss the Twitter API, we'll be using a special library called twython.\n",
    "# Twython has a lot of very useful functions and methods to help us access what is a very complicated interface\n",
    "# You don't need to reconnect constantly, so you can just use the Twython connection in other cells after running this cell.\n",
    "# Only exception is the streaming cell at the bottom\n",
    "# You must re-run this cell anytime you do Kernel->Restart or wait more than 5 minutes\n",
    "\n",
    "# Before running this cell for the first time, you'll need to enter the command \n",
    "# \"pip instal twython\" (no quotes) in a separate command prompt/terminal window first\n",
    "# This is so twython will be installed\n",
    "\n",
    "from twython import Twython\n",
    "import pprint\n",
    "\n",
    "\n",
    "# To access twitter, you need an app key and an app secret key. These can be obtained free from Twitter\n",
    "# Go to https://apps.twitter.com/, sign in/create an account, and click Create New App. Follow the steps inside\n",
    "# It will not function if you leave app_key and app_secret as-is!\n",
    "twython = Twython(app_key='YOUR_APP_KEY', app_secret='YOUR_APP_SECRET_KEY')\n",
    "\n",
    "print(\"Twitter API Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "followers = twython.get_followers_list(screen_name = \"ryanmcgrath\")\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547450\n",
      "955397\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "counter2 = 0\n",
    "for idx, city in enumerate(self_city):\n",
    "    value = df.get_value(idx, \"Self_City\")\n",
    "    value2 = df.get_value(idx, \"Self_State\")\n",
    "    if str(value2) != \"nan\":\n",
    "        counter2 += 1\n",
    "    if str(value2) != \"nan\" and str(value) != \"nan\":\n",
    "        counter += 1\n",
    "#df.to_csv(r\"C:\\Users\\joshl\\workspace\\PythonClass\\Lectures2\\Bad Libraries\\geolocated\\all_tweets_self_state_city_fixed2.csv\", encoding=\"ISO-8859-1\", index=False)\n",
    "print(counter)\n",
    "print(counter2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #1: Get user information for a list of users\n",
    "# Standard Rate Limit: 300 users per 15 minutes\n",
    "\n",
    "import datetime\n",
    "import delorean\n",
    "\n",
    "# Get a list of Twitter screen names\n",
    "congress_twitter_handles = [\"MacTXPress\",\"RepAdamSmith\",\"RepMikeTurner\"]\n",
    "\n",
    "# The meat - connect with twython to find the users\n",
    "users = twython.lookup_user(screen_name = congress_twitter_handles)\n",
    "\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "# Print some common information on the screen for every user\n",
    "for user in users:\n",
    "    print(user['description'])\n",
    "    print(user['geo_enabled'])\n",
    "    print(user['name'])\n",
    "    print(user['statuses_count'])\n",
    "    \n",
    "    # delorean.parse is useful because if it's in a list, we can sort the datetimes!\n",
    "    print(delorean.parse(user['created_at']).datetime)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Important Twitter User-object data:\n",
    "\n",
    "# user['created_at'] - When the account was first created\n",
    "# user['description'] - Description of account written by user\n",
    "# user['entities'] - Nested dictionary with information on websites the user associates\n",
    "# user['favourites_count'] - How man favourites there are - note stupid British spelling\n",
    "# user['friends_count'] - How many friends they have\n",
    "# user['geo_enalbed'] - Are they mappable when they tweet?\n",
    "# user['id'] - Their unique user ID number\n",
    "# user['name'] - Their real name, as they put it\n",
    "# user['statuses_count'] - The number of overall status updates that the user has made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #2: Get Tweets from a single user's timeline\n",
    "# Rate Limit: 300 Tweets per 15 minutes\n",
    "\n",
    "import pprint\n",
    "\n",
    "# The meat - Grab the twitter timeline for the given screen name.\n",
    "# Count naturally means how many of the most recent tweets. count=100 is thus the past 100 tweets\n",
    "\n",
    "smith_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=50)\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "\n",
    "#for tweet in smith_tweets:\n",
    "   # pprint.pprint(tweet)\n",
    "print(\"Done\")\n",
    "# Below are additional optional parameters that can be added to twython.get_user_timeline()\n",
    "\n",
    "# Optional parameter: exclude_replies - True or False. Excludes reply-Tweets\n",
    "# Optional parameter: count - Up to 200 at a time. Sets the number of Tweets to return\n",
    "# Optional parameter: since_id - Go forward in time from the since_id for this person (integer)\n",
    "# Optional parameter: max_id - Go backward in time from the max_id for this person (integer)\n",
    "# Optional parameter: exclude_replies - When set to True, it excludes Tweets sent as replies\n",
    "# Optional parameter: include_rts - When set to False, ignores all retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #3: Let's practice writing out critical tweet information to a file!\n",
    "# This uses the smith_tweets object created in the previous cell\n",
    "# Thus you naturally must RUN the above cell for this one to work!\n",
    "\n",
    "import unicodecsv as csv\n",
    "\n",
    "# Open your file\n",
    "test_file = open('tweet_file.csv','wb')\n",
    "csv_write_file = csv.writer(test_file)\n",
    "\n",
    "# Write the first row - column names\n",
    "csv_write_file.writerow(['Hashtags','id','text','user_mentions','created_at','user','in_reply_to'])\n",
    "\n",
    "# Loop through the tweets\n",
    "for tweet in smith_tweets:\n",
    "    # Advanced Trick - [join() + List Comprehension] Combo String Generator!\n",
    "    # Use List Comprehension to perform some useful data conversions\n",
    "    screen_name_mentions = \", \".join([mention['screen_name'] for mention in tweet['entities']['user_mentions']])\n",
    "    hashtags = \", \".join([mention['text'] for mention in tweet['entities']['hashtags']])\n",
    "    \n",
    "    \n",
    "    # Finally, write the row\n",
    "    current_row = [hashtags,tweet['id'], tweet['text'],screen_name_mentions,\n",
    "                   tweet['created_at'],tweet['user']['id'],tweet['in_reply_to_user_id']]\n",
    "    csv_write_file.writerow(current_row)\n",
    "test_file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "# Important Tweet-object data:\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['entities']['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #4: Do a search for tweets in the archive\n",
    "# Rate Limit: 450 per 15 minutes\n",
    "# Note: THIS LOOKS BACK IN TIME FROM THE PRESENT\n",
    "import time\n",
    "# Note 1: Count defaults to 15. 100 is the maximum per search\n",
    "# Note 2: max_id and since_id are valuable parameters, since id is a unique value \n",
    "# of a tweet and are in chronological order. Thus, since_id looks forward in time from the ID you give it\n",
    "# and max_id looks UP TO the ID you give it\n",
    "# Note 3: language is a best-guess attempt at determining a tweet's language.\n",
    "# Use the codes located in https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "\n",
    "# This will find me the last 100 tweets starting from the Present that contained the word Python in them and\n",
    "# are primarily in the English language\n",
    "\n",
    "results = twython.cursor(twython.search, q='Python',count=50, language='en')\n",
    "time.sleep(1)\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "\n",
    "for idx, tweet in enumerate(results):\n",
    "    pprint.pprint(tweet['text'])\n",
    "    pprint.pprint(tweet['user']['screen_name'])\n",
    "\n",
    "    #retweets = twython.get_retweets(id=tweet['id'])\n",
    "    # Nested for loop to print all retweets to a given tweet\n",
    "    #for retweet in retweets:\n",
    "        #print(\"RETWEET!!\")\n",
    "        #pprint.pprint(retweet)\n",
    "    # Let's only print out the first eight\n",
    "    if idx == 8:\n",
    "        break\n",
    "\n",
    "# Important Tweet-object data:\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADVANCED Task #1:\n",
    "# How do we get more than a few hundred tweets at a time?\n",
    "\n",
    "# Lets say we want RepAdamSmith's Tweets going back to October 12th, 2016 at 9:42 PM\n",
    "import delorean\n",
    "import pprint\n",
    "import unicodecsv as csv\n",
    "import time\n",
    "\n",
    "# Set an end date datetime object. Thus, grab tweets \"until\" this datetime\n",
    "end_date = delorean.parse(\"October 12th, 2016 9:42:01 PM\")\n",
    "\n",
    "# Next, make a True/False variable as a check to see if we've passed the date in question\n",
    "# It starts at False\n",
    "date_past = False\n",
    "all_tweets = []\n",
    "current_max_id = 0\n",
    "\n",
    "# Finally, the meat of the cell!\n",
    "while date_past == False:\n",
    "    # Try-Except is something new. It's something called \"error catching\"\n",
    "    # Basically, rather than have Python crash on an error, we want it to handle the error and continue processing\n",
    "    try:\n",
    "        # If this is the first ever request, max_id will be 0 and thus we want to get the most recent Tweets\n",
    "        if current_max_id == 0:\n",
    "            current_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=200)\n",
    "        # However, if we do have a max_id, we want to use it as a \"Starting Point\" for the next request\n",
    "        else:\n",
    "            current_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=200, max_id = current_max_id)\n",
    "            \n",
    "        # The next line is a useful method of extracting how many requests you have left in the current 15 minute window\n",
    "        requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "        print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "        \n",
    "        # Notice that I use += rather than .append(). += merges lists, whereas .append() makes a list of lists\n",
    "        all_tweets += current_tweets\n",
    "        \n",
    "    except TwythonRateLimitError as error: # This code is only called if the specific error noted occurs\n",
    "        # The code in here basically checks how much time Twitter wants you to wait, waits that long, and restarts\n",
    "        print(\"Error! Rate Limit has been hit!\")\n",
    "        # Get how much time Twitter demands we wait\n",
    "        remainder = float(twython.get_lastfunction_header(header='x-rate-limit-reset')) - time.time()\n",
    "        # Disconnect from Twitter\n",
    "        twython.disconnect()\n",
    "        print(\"Now waiting for: \" + str(remainder))\n",
    "        # Wait in sleep mode\n",
    "        time.sleep(remainder)\n",
    "        print(\"Waiting complete! Attempting reconnect...\")\n",
    "        # Reconnect\n",
    "        twython = Twython(app_key=my_twitter_info[0],app_secret=my_twitter_info[1],\n",
    "        oauth_token=my_twitter_info[2],oauth_token_secret=my_twitter_info[3])\n",
    "        # Go to the beginning of the while loop\n",
    "        continue\n",
    "    \n",
    "    # Remember, they go in reverse chronological order, so the last tweet is always the furthest back in time\n",
    "    last_tweet = current_tweets[(len(current_tweets)-1)]\n",
    "    \n",
    "    # Get the date of the LAST tweet in the current grab. This will be the one furthest in the past\n",
    "    current_date = delorean.parse(last_tweet['created_at'])\n",
    "    \n",
    "    # Get the Tweet ID of the LAST tweet in the current grab (once again, will be the one furthest in the past)\n",
    "    current_max_id = last_tweet['id']\n",
    "    \n",
    "    # Datetime check - if the current_date is further in the past than the end_date, end the while loop.\n",
    "    # This is why we use Delorean! Super-easy datetime comparision is awesome!\n",
    "    if current_date < end_date:\n",
    "        date_past = True\n",
    "\n",
    "# Once the looping is complete, we need to write our results\n",
    "# This should look familiar!\n",
    "# Open your file\n",
    "test_file = open('smith_tweets.csv','wb')\n",
    "csv_write_file = csv.writer(test_file)\n",
    "\n",
    "# Write the first row - the headers\n",
    "csv_write_file.writerow(['Hashtags','id','text','user_mentions','created_at','user','in_reply_to'])\n",
    "\n",
    "# Loop through the tweets\n",
    "for tweet in all_tweets:\n",
    "    # Advanced Trick - [join() + List Comprehension] Combo String Generator!\n",
    "    # Use List Comprehension to perform some useful data conversions\n",
    "    screen_name_mentions = \", \".join([mention['screen_name'] for mention in tweet['entities']['user_mentions']])\n",
    "    hashtags = \", \".join([mention['text'] for mention in tweet['entities']['hashtags']])\n",
    "    \n",
    "    \n",
    "    # Finally, write the row\n",
    "    current_row = [hashtags,tweet['id'], tweet['text'],screen_name_mentions,\n",
    "                   tweet['created_at'],tweet['user']['id'],tweet['in_reply_to_user_id']]\n",
    "    csv_write_file.writerow(current_row)\n",
    "test_file.close()\n",
    "print(\"All data written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADVANCED Task #2: Streaming tweets LIVE from across Twitter\n",
    "# Rate Limit: 1% of the \"tweets-per-second\" going on on the network\n",
    "# If you surpass that limit, it will only display a sample of the overall tweets\n",
    "# This may seem high, but you'd be amazed how easily you can reach it\n",
    "# NOTE: This looks forward in time from the present.\n",
    "# WARNING: Be careful about displaying your results on the screen - if you're streaming\n",
    "# more than a thousand tweets a minute (depending on how powerful your computer is), Jupyter Notebook may crash.\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "from timeit import default_timer as timer\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "gathered_tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    # Start a clock - only called when MyStreamer is initialized\n",
    "    start = timer()\n",
    "    \n",
    "    # What do you want Python to do each time Twitter informs it that a Tweet arrived?\n",
    "    # Maybe you want to write it to the screen? Maybe write it to a file.\n",
    "    def on_success(self, data):\n",
    "        # Set a timer for Right Now\n",
    "        current = timer()\n",
    "        \n",
    "        # Convert seconds to minutes\n",
    "        minutes_elapsed = round(current-self.start)/60\n",
    "        print(\"Current time running (in minutes) is: {0}\".format(minutes_elapsed))\n",
    "        \n",
    "        # Print out full nested dictionary\n",
    "        pprint.pprint(data['text'])\n",
    "        \n",
    "        # Add nested dictionary to gathered_tweets list\n",
    "        gathered_tweets.append(data)\n",
    "        \n",
    "        if minutes_elapsed > 1:\n",
    "            sys.exit(0)\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        # If you get Error Code: 406, it means you're using screen names, NOT ids!\n",
    "        print (\"Error Code: \" + str(status_code))\n",
    "        self.disconnect()\n",
    "\n",
    "# Notice I'm creating a MyStreamer object. For the Streamer, you also need to have\n",
    "# something called an OAuth token and its secret key. These can also be obtained\n",
    "# from Twitter, free of charge once you have an account.\n",
    "streamer = MyStreamer(app_key='YOUR_APP_KEY',\n",
    "        app_secret='YOUR_APP_SECRET_KEY',\n",
    "        oauth_token='YOUR_OAUTH_TOKEN',\n",
    "        oauth_token_secret='YOUR_OAUTH_TOKEN_SECRET')\n",
    "\n",
    "# How to actually begin the streamer:\n",
    "\n",
    "# Track tweet stream by text-matching. Can be used for hashtag matching\n",
    "#streamer.statuses.filter(track='Python')\n",
    "\n",
    "# Get all French tweets with the word 'the'\n",
    "#streamer.statuses.filter(track=['the','and'],language='en',stall_warnings=True, filter_level='medium')\n",
    "\n",
    "# congress_twitter_handles = [\"MacTXPress\",\"RepAdamSmith\",\"RepMikeTurner\"]\n",
    "# streamer.statuses.filter(follow=congress_twitter_handles)\n",
    "\n",
    "# Here's an example:\n",
    "streamer.statuses.filter(locations='-122.75,36.8,-121.75,37.8,-74,40,-73,41', \n",
    "                         language='en') # English tweets from New York or San Francisco\n",
    "\n",
    "# Important Tweet Data\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
