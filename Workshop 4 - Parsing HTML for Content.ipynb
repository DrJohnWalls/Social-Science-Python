{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_variable = None\n",
    "the_variable =\n",
    "if my_variable:\n",
    "    print(\"Hello!\")\n",
    "else:\n",
    "    print(\"Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once we download an HTML file, how do we extract the data we want? A single webpage can easily be 20 pages worth of\n",
    "# HTML code, and oftentimes up to 100 pages. Thus, simply downloading the webpage code and copy-pasting is horribly\n",
    "# inefficient.\n",
    "\n",
    "# Let's do this again\n",
    "import requests\n",
    "\n",
    "my_response_object = requests.get(\"http://mason.gmu.edu/~jlee17/python_workshop_files/example_data/index-very-simple.html\")\n",
    "html_text = my_response_object.text\n",
    "print(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Jupyter Notebook comes with the all-powerful bs4 library. By convention, just importing the BeautifulSoup function\n",
    "# is considered the simplest method. Thus the syntax from .... import ....\n",
    "# Without this syntax (if I just did import bs4), we'd need bs4.BeautifulSoup() instead of BeautifulSoup()\n",
    "# Maybe a bit silly here, but this is the standard convention.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Let's transform it into SOUP!\n",
    "# There are two parameters. The first, naturally, is our html text.\n",
    "# The second is which \"parser\" we'll be using. For now, html.parser is fine. You won't really ever\n",
    "# need to change that.\n",
    "soup = BeautifulSoup(html_text,\"html.parser\")\n",
    "\n",
    "# soup objects have a really nifty prettify() method\n",
    "print(soup.prettify())\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I wonder what else is inside our soup object?\n",
    "# soup objects have lots of variables inside them, with information set to them by default.\n",
    "# title gets the <title> element in its entirety\n",
    "\n",
    "print(\"Here is the title, tag PLUS text: {0}\".format(soup.title))\n",
    "# <title>A Sample Title</title>\n",
    "\n",
    "# If you just want the name of the element, you can get 'name' attribute.\n",
    "print(\"Here is the title, tag ONLY: {0}\".format(soup.title.name))\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More basics\n",
    "print(\"Here is the title tag's PARENT's tag NAME: {0}\".format(soup.title.parent.name))\n",
    "# 'head'\n",
    "\n",
    "print(\"Here is the first entire 'p' element: {0}\".format(soup.p))\n",
    "# <p>Here is a paragraph. But it doesn't have any attributes that you can capture!</p>\n",
    "\n",
    "#print(\"Here is the value of the id attribuate for the first 'div' element: {0}\".format(soup.div['id']))\n",
    "# all_the_lists\n",
    "\n",
    "print(\"Here is the first entire 'a' element: {0}\".format(soup.a))\n",
    "# <a href=\"http://www.google.com\">Google</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different between .text and .string\n",
    "# .string just gets what in that element's immediate text area\n",
    "# .text is far more complete, it combines the current element plus all subelements together and merges all\n",
    "# their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup.title is all well and good, because there's only ever ONE title element in an HTML page. But what about\n",
    "# an element like <a>, which can appear many many times (and is very important to us?)\n",
    "# Doing soup.a is cute when you only care about the very first hyperlink. But HTML can get REALLY dense!\n",
    "# What do we do when we want elements that are really deep quickly?\n",
    "# Remember: every node (element) in the soup tree contains every method itself! What do I mean by that?\n",
    "\n",
    "# Let's get all the elements with an 'a' tag and put them in a list for us\n",
    "# All soup objects have a find_all method, which searches all elements BELOW that element in the tree\n",
    "# This finds ALL A TAGS in the ENTIRE DOCUMENT\n",
    "all_a_tags = soup.find_all('a')\n",
    "\n",
    "# It returns something called a ResultSet, which we can treat just like a list in terms of using a for loop\n",
    "# Let's loop around and see what's inside?\n",
    "for a_tag in all_a_tags:\n",
    "    print(\"----------------\")\n",
    "    # Let's see the whole tag\n",
    "    print(a_tag)\n",
    "    # Just the href attribute of that tag\n",
    "    print(a_tag['href'])\n",
    "    # The text of the tag\n",
    "    print(a_tag.string)\n",
    "    # The tag's name itself (a)\n",
    "    print(a_tag.name)\n",
    "\n",
    "# Example:\n",
    "# <a href=\"http://www.google.com\">Google</a>\n",
    "# http://www.google.com\n",
    "# Google\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_a = soup.find('a')\n",
    "print(my_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note find vs find_all - 'find' just finds the FIRST match below the root from top to bottom\n",
    "# 'find_all' finds (obviously) all matches below the root from top to bottom\n",
    "# However, id is always unique, so it's pretty safe to do find with it\n",
    "\n",
    "required_tags = {}\n",
    "required_tags['id'] = 'file_links'\n",
    "\n",
    "book_title_element = soup.find_all(\"div\", required_tags)\n",
    "\n",
    "for div_tag in book_title_element:\n",
    "    print(div_tag.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, so how might we find only a select group of hyperlinks from ALL the hyperlinks?\n",
    "# In this case, JUST the hyperlinks in the first div?\n",
    "# One common way is method chaining a find_all() after a find() for the div/ul element\n",
    "# Also note that if you're only selecting by a single attribute(such as ID here), you can just insert the\n",
    "# dictionary right in there if you like.\n",
    "ul_tag = soup.find(\"ul\",{\"id\":\"the_first_list\"})\n",
    "a_elements = ul_tag.find_all(\"a\")\n",
    "\n",
    "response = requests.get(\"http://whatever.com\")\n",
    "my_pdf = response.content\n",
    "my_file.write(my_pdf)\n",
    "for a_element in a_elements:\n",
    "    print(a_element['href'])\n",
    "    print(a_element.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Advanced Concept - Lambda Functions!\n",
    "\n",
    "# Lambda functions take some explaining. Basically, they're miniature, one-time-use functions that have no name.\n",
    "# They pass in a variable L (or whatever you want to name it), which is defined by BeautifulSoup in this case to be a string.\n",
    "# In this case, we want it to return TRUE if it's a match but FALSE if it's not a match\n",
    "# Imagine a for loop, looping through every 'ul' element tag in the file:\n",
    "\n",
    "# all_the_ul_tags = soup.find_all(\"ul\")\n",
    "# selected_ul_elements = []\n",
    "# for L in all_the_ul_tags:\n",
    "#     if L['id'] and L['id'].endswith(\"_list\"):\n",
    "#         selected_ul_elements.append(L['id'])\n",
    "\n",
    "# This is what the lambda function below is doing. Five lines of code into one line! Much cleaner, once you get the syntax \n",
    "# The BeautifulSoup package understands to make a list of only those TRUE elements\n",
    "# Also, the reason for the strange \"L and\" and the beginning is a fancy way of preventing errors.\n",
    "# All this line of code below does is say:\n",
    "# \"Find me all the 'ul' elements whose id attribute ends with \"_list\".\n",
    "# The \"L and\" at the beginning is a fancy way of making sure the id exists - without that,\n",
    "# if the ul didn't have an id attribute Python would produce an error.\n",
    "\n",
    "selected_ul_elements = soup.find_all(\"ul\",{'id' : lambda L: L and L.endswith(\"_list\") })\n",
    "\n",
    "for element in selected_ul_elements:\n",
    "    print(\"-----------------------\")\n",
    "    print(element)\n",
    "    print(\"-----------------------\")\n",
    "    print(element.find_all(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another tool you'll find useful is urljoin()\n",
    "# Oftentimes you get what are known as relative links, which need to be joined with the base link to make a full hyperlink\n",
    "# The cool part about urljoin() (vs. string concatenation) is that it doesn't care about making sure the slashes match\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "my_link = \"http://www.google.com/\" + \"/some_random_page.html\"\n",
    "print(my_link)\n",
    "print(urljoin(\"http://www.google.com\",\"some_random_page.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import unicodecsv as csv\n",
    "import progressbar\n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "my_file = open(\"clearance_stuff_3.csv\",\"wb\")\n",
    "my_ss = csv.writer(my_file)\n",
    "\n",
    "for value in bar(range(1996,2017)):\n",
    "    hyperlink = 'http://ogc.osd.mil/doha/industrial/' + str(value) + '.html'\n",
    "    response = requests.get(hyperlink)\n",
    "    assert response.status_code == 200\n",
    "    the_html = response.text\n",
    "    soup = BeautifulSoup(the_html,\"html.parser\")\n",
    "    my_dictionary = {}\n",
    "    my_dictionary['class'] = 'case-list'\n",
    "    my_main_div = soup.find(\"div\",my_dictionary)\n",
    "    all_the_cases = my_main_div.find_all(\"div\",{'class':'case'})\n",
    "    base_link = \"http://www.dod.mil\"\n",
    "    \n",
    "    for current_case in all_the_cases:\n",
    "        case_number = current_case.find(\"div\",{'class':'casenum'})\n",
    "        case_number_text = case_number.text\n",
    "        a_tag = case_number.find('a')\n",
    "        \n",
    "        if a_tag:\n",
    "            a_tag_link = urljoin(base_link,a_tag['href'])\n",
    "        else:\n",
    "            a_tag_link = base_link\n",
    "        keywords = current_case.find(\"div\",{'class':'keywords'})\n",
    "        keywords_text = keywords.text\n",
    "        if \";\" in keywords_text:\n",
    "            keyword_list = keywords_text.split(\";\")\n",
    "        elif \",\" in keywords_text:\n",
    "            keyword_list = keywords_text.split(\",\")\n",
    "        else:\n",
    "            keyword_list = [keywords_text]\n",
    "        date = current_case.find(\"p\",{'class':'date'})\n",
    "        date_text = date.text\n",
    "        summary = current_case.find(\"p\",{'class':'digest'})\n",
    "        summary_text = summary.text.strip()\n",
    "        my_ss.writerow([case_number_text,keywords_text,date_text,summary_text,a_tag_link])\n",
    "\n",
    "my_file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pprint(my_dict['stats'][0]['effort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import unicodecsv as csv\n",
    "file = open(\"pokemon.csv\",\"wb\")\n",
    "csv_file = csv.writer(file)\n",
    "csv_file.writerow(['Name','Weight','Speed','Defense','Attack','HP'])\n",
    "for idx in range(1,152):\n",
    "    response = requests.get(\"http://pokeapi.co/api/v2/pokemon/\" + str(idx))\n",
    "    assert response.status_code == 200\n",
    "    my_dict = response.json()\n",
    "    csv_file.writerow([my_dict['name'],my_dict['weight'],\n",
    "                       my_dict['stats'][0]['base_stat'],\n",
    "                       my_dict['stats'][3]['base_stat'],\n",
    "                     my_dict['stats'][4]['base_stat'],\n",
    "                       my_dict['stats'][5]['base_stat']])\n",
    "file.close()\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
