{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Configure regular Twitter API\n",
    "# Note that this cell has been separated - if you reconnect constantly, Twitter will get pissy with you\n",
    "# You don't need to reconnect constantly, so you can just use the twython connection in other cells after running this cell.\n",
    "# Only exception is the streaming cell at the bottom\n",
    "# You must re-run this cell anytime you do Kernel->Restart or wait more than 5 minutes\n",
    "# You also might need to enter the command \"pip instal twython\" (no quotes) in a separate command prompt/terminal window first\n",
    "# This is so twython will be installed\n",
    "\n",
    "from twython import Twython\n",
    "import pprint\n",
    "\n",
    "# This twython object represents our connection to Twitter via Python. By using methods housed within this object,\n",
    "# we can grab whatever we want from Twitter\n",
    "# You have to get the API keys yourself by making an account on Twitter.\n",
    "# Once you have a Twitter account, go to dev.twitter.com and create a \"new app\". Just fill it out with dummy information\n",
    "# If you look around, you should be able to find your app key and your app secret key.\n",
    "twython = Twython(app_key='YOUR-API-KEY-HERE',\n",
    "        app_secret='YOUR-API-SECRET-KEY-HERE')\n",
    "\n",
    "print(\"Twitter API Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #1: Get user information for a list of users\n",
    "# Standard Rate Limit: 300 users per 15 minutes\n",
    "\n",
    "import datetime\n",
    "import delorean # May need to do \"pip install delorean\" (without quotes) to install\n",
    "\n",
    "# Get a list of Twitter screen names\n",
    "congress_twitter_handles = [\"MacTXPress\",\"RepAdamSmith\",\"RepMikeTurner\"]\n",
    "\n",
    "# The meat - connect with twython to find the users\n",
    "users = twython.lookup_user(screen_name = congress_twitter_handles)\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "# Print some common information on the screen for every user\n",
    "for user in users:\n",
    "    print(user['description'])\n",
    "    print(user['geo_enabled'])\n",
    "    print(user['name'])\n",
    "    print(user['statuses_count'])\n",
    "    \n",
    "    # delorean.parse is useful because if it's in a list, we can sort the datetimes!\n",
    "    print(delorean.parse(user['created_at']).datetime)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Important Twitter User-object data:\n",
    "\n",
    "# user['created_at'] - When the account was first created\n",
    "# user['description'] - Description of account written by user\n",
    "# user['entities'] - Nested dictionary with information on websites the user associates\n",
    "# user['favourites_count'] - How man favourites there are - note stupid British spelling\n",
    "# user['friends_count'] - How many friends they have\n",
    "# user['geo_enalbed'] - Are they mappable when they tweet?\n",
    "# user['id'] - Their unique user ID number\n",
    "# user['name'] - Their real name, as they put it\n",
    "# user['statuses_count'] - The number of overall status updates that the user has made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task #2: Get Tweets from a single user's timeline\n",
    "# Rate Limit: 300 Tweets per 15 minutes\n",
    "\n",
    "import pprint\n",
    "\n",
    "# The meat - Grab the twitter timeline for the given screen name.\n",
    "# Count naturally means how many of the most recent tweets. count=100 is thus the past 100 tweets\n",
    "smith_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=100)\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "\n",
    "for tweet in smith_tweets:\n",
    "    pprint.pprint(tweet)\n",
    "\n",
    "# Below are additional optional parameters that can be added to twython.get_user_timeline()\n",
    "\n",
    "# Optional parameter: exclude_replies - True or False. Excludes reply-Tweets\n",
    "# Optional parameter: count - Up to 200 at a time. Sets the number of Tweets to return\n",
    "# Optional parameter: since_id - Go forward in time from the since_id for this person (integer)\n",
    "# Optional parameter: max_id - Go backward in time from the max_id for this person (integer)\n",
    "# Optional parameter: exclude_replies - When set to True, it excludes Tweets sent as replies\n",
    "# Optional parameter: include_rts - When set to False, ignores all retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #3: Let's practice writing out critical tweet information to a file!\n",
    "# This uses the smith_tweets object created in the previous cell\n",
    "# Thus you naturally must RUN the above cell for this one to work!\n",
    "\n",
    "import unicodecsv as csv\n",
    "\n",
    "# Open your file\n",
    "test_file = open('tweet_file.csv','wb')\n",
    "csv_write_file = csv.writer(test_file)\n",
    "\n",
    "# Write the first row - column names\n",
    "csv_write_file.writerow(['Hashtags','id','text','user_mentions','created_at','user','in_reply_to'])\n",
    "\n",
    "# Loop through the tweets\n",
    "for tweet in smith_tweets:\n",
    "    # Advanced Trick - [join() + List Comprehension] Combo String Generator!\n",
    "    # Use List Comprehension to perform some useful data conversions\n",
    "    screen_name_mentions = \", \".join([mention['screen_name'] for mention in tweet['entities']['user_mentions']])\n",
    "    hashtags = \", \".join([mention['text'] for mention in tweet['entities']['hashtags']])\n",
    "    \n",
    "    \n",
    "    # Finally, write the row\n",
    "    current_row = [hashtags,tweet['id'], tweet['text'],screen_name_mentions,\n",
    "                   tweet['created_at'],tweet['user']['id'],tweet['in_reply_to_user_id']]\n",
    "    csv_write_file.writerow(current_row)\n",
    "test_file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "# Important Tweet-object data:\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['entities']['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #4: Do a search for tweets in the archive\n",
    "# Rate Limit: 450 per 15 minutes\n",
    "# Note: THIS LOOKS BACK IN TIME FROM THE PRESENT\n",
    "import time\n",
    "# Note 1: Count defaults to 15. 100 is the maximum per search\n",
    "# Note 2: max_id and since_id are valuable parameters, since id is a unique value \n",
    "# of a tweet and are in chronological order. Thus, since_id looks forward in time from the ID you give it\n",
    "# and max_id looks UP TO the ID you give it\n",
    "# Note 3: language is a best-guess attempt at determining a tweet's language.\n",
    "# Use the codes located in https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "\n",
    "# This will find me the last 10 tweets starting from the Present that contained the word Python in them and\n",
    "# are primarily in the English language\n",
    "results = twython.cursor(twython.search, q='Python',count=10, language='en')\n",
    "time.sleep(1)\n",
    "requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "\n",
    "for idx, tweet in enumerate(results):\n",
    "    pprint.pprint(tweet['text'])\n",
    "    pprint.pprint(tweet['user']['screen_name'])\n",
    "\n",
    "    #retweets = twython.get_retweets(id=tweet['id'])\n",
    "    # Nested for loop to print all retweets to a given tweet\n",
    "    #for retweet in retweets:\n",
    "        #print(\"RETWEET!!\")\n",
    "        #pprint.pprint(retweet)\n",
    "    # Let's only print out the first eight\n",
    "    if idx == 8:\n",
    "        break\n",
    "\n",
    "# Important Tweet-object data:\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task #5: What if I need more than a few hundred tweets?\n",
    "# How do we make Python continually get tweets until a certain point?\n",
    "\n",
    "# Lets say we want RepAdamSmith's Tweets going back to October 12th, 2015 at 9:42 PM\n",
    "import delorean\n",
    "import pprint\n",
    "import unicodecsv as csv\n",
    "import time\n",
    "\n",
    "# Set an end date datetime object. Thus, grab tweets \"until\" this datetime\n",
    "end_date = delorean.parse(\"October 12th, 2011 9:42 PM\")\n",
    "\n",
    "# Next, make a True/False variable as a check to see if we've passed the date in question\n",
    "# It starts at False\n",
    "date_past = False\n",
    "all_tweets = []\n",
    "current_max_id = 0\n",
    "\n",
    "# Finally, the meat of the cell!\n",
    "while date_past == False:\n",
    "    # Try-Except is something new. It's something called \"error catching\"\n",
    "    # Basically, rather than have Python crash on an error, we want it to handle the error and continue processing\n",
    "    try:\n",
    "        # If this is the first ever request, max_id will be 0 and thus we want to get the most recent Tweets\n",
    "        if current_max_id == 0:\n",
    "            current_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=200)\n",
    "        # However, if we do have a max_id, we want to use it as a \"Starting Point\" for the next request\n",
    "        else:\n",
    "            current_tweets = twython.get_user_timeline(screen_name='RepAdamSmith', count=200, max_id = current_max_id)\n",
    "            \n",
    "        # The next line is a useful method of extracting how many requests you have left in the current 15 minute window\n",
    "        requests_until_limit = str(twython.get_lastfunction_header(header='X-Rate-Limit-Remaining'))\n",
    "        print(\"You have: \" + requests_until_limit + \" requests remaining in this 15 minute window\")\n",
    "        \n",
    "        # Notice that I use += rather than .append(). += merges lists, whereas .append() makes a list of lists\n",
    "        all_tweets += current_tweets\n",
    "        \n",
    "    except TwythonRateLimitError as error: # This code is only called if the specific error noted occurs\n",
    "        # The code in here basically checks how much time Twitter wants you to wait, waits that long, and restarts\n",
    "        print(\"Error! Rate Limit has been hit!\")\n",
    "        # Get how much time Twitter demands we wait\n",
    "        remainder = float(twython.get_lastfunction_header(header='x-rate-limit-reset')) - time.time()\n",
    "        # Disconnect from Twitter\n",
    "        twython.disconnect()\n",
    "        print(\"Now waiting for: \" + str(remainder))\n",
    "        # Wait in sleep mode\n",
    "        time.sleep(remainder)\n",
    "        print(\"Waiting complete! Attempting reconnect...\")\n",
    "        # Reconnect\n",
    "        twython = Twython(app_key=my_twitter_info[0],app_secret=my_twitter_info[1],\n",
    "        oauth_token=my_twitter_info[2],oauth_token_secret=my_twitter_info[3])\n",
    "        # Go to the beginning of the while loop\n",
    "        continue\n",
    "    \n",
    "    # Remember, they go in reverse chronological order, so the last tweet is always the furthest back in time\n",
    "    last_tweet = current_tweets[(len(current_tweets)-1)]\n",
    "    \n",
    "    # Get the date of the LAST tweet in the current grab. This will be the one furthest in the past\n",
    "    current_date = delorean.parse(last_tweet['created_at'])\n",
    "    \n",
    "    # Get the Tweet ID of the LAST tweet in the current grab (once again, will be the one furthest in the past)\n",
    "    current_max_id = last_tweet['id']\n",
    "    \n",
    "    # Datetime check - if the current_date is further in the past than the end_date, end the while loop.\n",
    "    # This is why we use Delorean! Super-easy datetime comparision is awesome!\n",
    "    if current_date < end_date:\n",
    "        date_past = True\n",
    "\n",
    "# Once the looping is complete, we need to write our results\n",
    "# This should look familiar!\n",
    "# Open your file\n",
    "test_file = open('smith_tweets.csv','wb')\n",
    "csv_write_file = csv.writer(test_file)\n",
    "\n",
    "# Write the first row - the headers\n",
    "csv_write_file.writerow(['Hashtags','id','text','user_mentions','created_at','user','in_reply_to'])\n",
    "\n",
    "# Loop through the tweets\n",
    "for tweet in all_tweets:\n",
    "    # Advanced Trick - [join() + List Comprehension] Combo String Generator!\n",
    "    # Use List Comprehension to perform some useful data conversions\n",
    "    screen_name_mentions = \", \".join([mention['screen_name'] for mention in tweet['entities']['user_mentions']])\n",
    "    hashtags = \", \".join([mention['text'] for mention in tweet['entities']['hashtags']])\n",
    "    \n",
    "    \n",
    "    # Finally, write the row\n",
    "    current_row = [hashtags,tweet['id'], tweet['text'],screen_name_mentions,\n",
    "                   tweet['created_at'],tweet['user']['id'],tweet['in_reply_to_user_id']]\n",
    "    csv_write_file.writerow(current_row)\n",
    "test_file.close()\n",
    "print(\"All data written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task #6: Constantly stream tweets (ADVANCED!!)\n",
    "# Rate Limit: 1% of the \"tweets-per-second\" going on on the network\n",
    "# This may seem high, but you'd be amazed how easily you can reach it\n",
    "# NOTE: THIS LOOKS FORWARD IN TIME FROM THE PRESENT\n",
    "# WARNING: BE CAREFUL ABOUT DISPLAYING YOUR RESULTS ON THE SCREEN IF YOU'RE STREAMING MORE THAN A FEW HUNDRED TWEETS!!\n",
    "# JUPYTER NOTEBOOK WILL CRASH!!!\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "from timeit import default_timer as timer\n",
    "import aespy\n",
    "import atexit\n",
    "import pprint\n",
    "\n",
    "# Safety precaution - Destroy all password data upon script exiting from the computer's memory\n",
    "def destroy_passwords():\n",
    "    del my_twitter_info\n",
    "    print(\"Deleted!\")\n",
    "atexit.register(destroy_passwords)\n",
    "\n",
    "gathered_tweets = []\n",
    "\n",
    "class JoshStreamer(TwythonStreamer):\n",
    "    # Start a clock - only called when JoshStreamer is initialized\n",
    "    start = timer()\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        # Set a timer for Right Now\n",
    "        current = timer()\n",
    "        \n",
    "        # Convert seconds to minutes\n",
    "        minutes_elapsed = round(current-self.start)/60\n",
    "        print(\"Current time running (in minutes) is: {0}\".format(minutes_elapsed))\n",
    "        \n",
    "        # Print out full nested dictionary\n",
    "        pprint.pprint(data)\n",
    "        \n",
    "        # Add nested dictionary to gathered_tweets list\n",
    "        gathered_tweets.append(data)\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        # If you get Error Code: 406, it means you're using screen names, NOT ids!\n",
    "        print (\"Error Code: \" + str(status_code))\n",
    "        self.disconnect()\n",
    "\n",
    "with open(\"password_file.txt\",\"r\") as my_pass_file:\n",
    "    my_twitter_info = [password.replace(\"\\n\",\"\") for password in my_pass_file.readlines()]\n",
    "\n",
    "# Notice I'm creating a JoshStreamer object, NOT a generic Twython Object like above\n",
    "streamer = JoshStreamer(app_key=my_twitter_info[0],\n",
    "        app_secret=my_twitter_info[1],\n",
    "        oauth_token=my_twitter_info[2],\n",
    "        oauth_token_secret=my_twitter_info[3])\n",
    "\n",
    "# Clear Twitter password info\n",
    "my_twitter_info = ''\n",
    "\n",
    "# How to actually begin the streamer:\n",
    "# CRITICAL WARNING: THE THREE PRIMARY FILTERS (track/follow/location) USE <OR>, NOT <AND>\n",
    "# Thus, using two \"primary filters\" at once is usually a bad idea\n",
    "# Example: if you use streamer.statuses.filter(track='Python',follow=congress_twitter_handle)\n",
    "# it will not grab all tweets containing the text Python AND following those congressmen\n",
    "# Rather it will grab all tweets containing the text Python OR following those congressmen\n",
    "# Idiotic, I know.\n",
    "# The proper way to further filter is to filter from within the on_success function\n",
    "\n",
    "# Track tweet stream by text-matching. Can be used for hashtag matching\n",
    "#streamer.statuses.filter(track='Python')\n",
    "\n",
    "# Get all French tweets with the word 'the'\n",
    "streamer.statuses.filter(track='the',language='en',stall_warnings=True, filter_level='medium')\n",
    "\n",
    "# congress_twitter_handles = [\"MacTXPress\",\"RepAdamSmith\",\"RepMikeTurner\"]\n",
    "# streamer.statuses.filter(follow=congress_twitter_handles)\n",
    "\n",
    "# WARNING  - WILL ONLY WORK FOR USERS WITH GEOTAGGING ENABLED!\n",
    "# streamer.statuses.filter(locations='-122.75,36.8,-121.75,37.8,-74,40,-73,41') # New York or San Francisco\n",
    "\n",
    "# Important Twitter Tweet-Object Data\n",
    "\n",
    "# tweet['entities']['hashtags'] - The hashtags used in this specific tweet, organized in a Python list\n",
    "# tweet['id'] - The tweet's unique ID number\n",
    "# tweet['place']['bounding_box']['coordinates'] - A list of GPS lat-lon pairs forming a \"bounding box\"\n",
    "# tweet['text'] - The actual text of the tweet\n",
    "# tweet['user_mentions'] - A LIST of DICTIONARIES containing information about the users mentioned in the tweet\n",
    "# tweet['created_at'] - The date/time the tweet was created at.\n",
    "# tweet['user'] - A DICTIONARY containing all the user's information who tweeted this tweet.\n",
    "# tweet['in_reply_to_status_id'] - Tweet ID if Tweet was in reply; None otherwise\n",
    "# tweet['in_reply_to_user_id'] - User ID if Tweet was in reply; None otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, useful Trick: Convert from string literal to dictionary\n",
    "# Maybe you saved tweet output dictionary to a text file, for example\n",
    "import ast\n",
    "import pprint\n",
    "my_dict = ast.literal_eval(\"{'muffin' : 'lolz', 'foo' : 'kitty'}\")\n",
    "pprint.pprint(my_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
