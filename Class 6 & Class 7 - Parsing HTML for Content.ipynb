{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CLASS 6 + 7: Parsing HTML for specific content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once we download an HTML file, how do we extract the data we want? A single webpage can easily be 20 pages worth of\n",
    "# HTML code, and oftentimes up to 100 pages. Thus, simply downloading the webpage code and copy-pasting or using Ctrl + F \n",
    "# is horribly inefficient.\n",
    "\n",
    "# Let's do this again\n",
    "import requests\n",
    "\n",
    "my_response_object = requests.get(\"http://mason.gmu.edu/~jlee17/python_workshop_files/example_data/index-very-simple.html\")\n",
    "html_text = my_response_object.text\n",
    "print(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bs4 (aka BeautifulSoup) is an amazingly powerful method of parsing HTML files\n",
    "# The syntax [from .... import ....] is used when you only want certain functions.\n",
    "# Without this syntax (that is, if we just did [import bs4]), we'd need bs4.BeautifulSoup() instead of BeautifulSoup()\n",
    "# Maybe a bit silly, but this is the standard naming convention in Python for the bs4 library.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Let's transform our text into a Soup object. \n",
    "# The \"lxml\" parameter there just specifies how Python should parse it. \n",
    "# lxml is a particularly good parser, but it may not be available on every operating system and version.\n",
    "# If your computer doesn't already have it, go to your command prompt and type in \"pip install lxml\" (no quotes)\n",
    "# If that doesn't work, you can replace \"lxml\" with \"html.parse\", which Python has by default.\n",
    "soup = BeautifulSoup(html_text,\"lxml\")\n",
    "\n",
    "# You never need pprint with soup objects - they have their own prettify() method\n",
    "print(soup.prettify())\n",
    "# Note that prettify() is specially designed for HTML tags - it helps you see what tags are inside of what\n",
    "# other tags.\n",
    "\n",
    "# You can also just grab out all the text and rip out all the HTML tags by using the .text attribute:\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I wonder what else is inside our soup object? We can not only print it and display the text, but there's also\n",
    "# a whole universe of functionality that we can apply to soup objects, especially for more complex pages.\n",
    "# Soup objects are made up of NODES.\n",
    "# Each node represents an HTML tag in its entirety, including all the tags that are inside of it.\n",
    "# The original Soup object is itself a node. It is the \"root node\", with all other nodes being its children, grandchildren, etc.\n",
    "\n",
    "# Below, the title attribute gets the <title> NODE in its entirety from the document\n",
    "\n",
    "# Also note the special {0} syntax in the string - this can be simpler than simply doing\n",
    "# \"the\" + variable + \" other string\" every time. You can use {0}, {1}, etc. for multiple variables\n",
    "print(\"Here is the title, tag AND text: {0}\".format(soup.title))\n",
    "# <title>A Sample Title</title>\n",
    "\n",
    "# If you just want the name of the element, you can get 'name' attribute from the 'title' attribute.\n",
    "print(\"Here is the title, tag ONLY: {0}\".format(soup.title.name))\n",
    "# 'title'\n",
    "\n",
    "print(\"Here is the title, text ONLY: {0}\".format(soup.title.string))\n",
    "# u'A Sample Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More basics\n",
    "print(\"Here is the title tag's PARENT's tag NAME: {0}\".format(soup.title.parent.name))\n",
    "# 'head'\n",
    "\n",
    "print(\"Here is the first entire 'p' element: {0}\".format(soup.p))\n",
    "# <p>Here is a paragraph. But it doesn't have any attributes that you can capture!</p>\n",
    "\n",
    "# When you want to get HTML attributes (not to be confused with Python's object attributes)\n",
    "# you treat the HTML node like a dictionary. Try to follow along with what the code below is grabbing\n",
    "print(\"Here is the value of the id attribuate for the first 'div' element: {0}\".format(soup.div['id']))\n",
    "# all_the_lists\n",
    "\n",
    "print(\"Here is the first entire 'a' element: {0}\".format(soup.a))\n",
    "# <a href=\"http://www.google.com\">Google</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different between .text and .string\n",
    "# .string just gets what in that element's immediate text area\n",
    "# .text is far more complete, it combines the current element plus all subelements together and merges all\n",
    "# their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup.title is all well and good, because there's only ever ONE title element in an HTML page. \n",
    "# But what about an element like <a>, which can appear many many times (and is very important)?\n",
    "# Doing soup.a is ok when you only care about the very first hyperlink. \n",
    "# But HTML can get really dense. What do you do when you want elements that are deep in the text quickly?\n",
    "\n",
    "# Rather than just the first, let's get ALL the elements with an 'a' tag and put them in a list for us\n",
    "# All soup objects (i.e. nodes) have a find_all method, which searches all nodes INSIDE that node in the tree\n",
    "# If you call find_all on the root node (soup), it will thus search the entire document.\n",
    "all_a_tags = soup.find_all('a')\n",
    "\n",
    "# It returns a ResultSet object, which we can treat just like a List\n",
    "# Let's loop through it and see what's inside?\n",
    "for a_tag in all_a_tags:\n",
    "    print(\"----------------\")\n",
    "    # Let's see the whole thing\n",
    "    print(a_tag)\n",
    "    # What about just the 'href' HTML attribute?\n",
    "    print(a_tag['href'])\n",
    "    # The text of the tag?\n",
    "    print(a_tag.string)\n",
    "    # The tag's name itself?\n",
    "    print(a_tag.name)\n",
    "\n",
    "# Example Output:\n",
    "# <a href=\"http://www.google.com\">Google</a>\n",
    "# http://www.google.com\n",
    "# Google\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## find vs find_all\n",
    "# 'find' just finds the FIRST match inside current node.\n",
    "# 'find_all' finds ALL matches inside the current node.\n",
    "\n",
    "# Just like with requests.get(), you can attach a dictionary on the end of the find() or find_all() method\n",
    "# That dictionary contains HTML attributes(s) that the node also must match.\n",
    "\n",
    "required_tags = {}\n",
    "required_tags['id'] = 'file_links'\n",
    "\n",
    "# Thus, below, instead of just finding \"the first div tag\" in the soup object\n",
    "# We're finding \"the first div tag that ALSO has an 'id' attribute equal to 'file_links'\" in the soup object\n",
    "book_title_element = soup.find(\"div\",required_tags)\n",
    "\n",
    "print(type(book_title_element))\n",
    "print(book_title_element.prettify())\n",
    "print(book_title_element['id'])\n",
    "print(book_title_element.name)\n",
    "print(book_title_element.string)\n",
    "print(book_title_element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, so how might we find only a select group of hyperlinks from all the hyperlinks?\n",
    "# In this case, just the hyperlinks in the first div?\n",
    "\n",
    "# Also, note that if you're only selecting by a single attribute(such as ID here), you can just insert the\n",
    "# dictionary right in without putting it in a variable first.\n",
    "\n",
    "# First, get the proper div tag\n",
    "div_tag = soup.find(\"ul\",{\"id\":\"the_first_list\"})\n",
    "\n",
    "# Then, from that div tag, get all the 'a' tags INSIDE the div tag.\n",
    "a_elements = div_tag.find_all(\"a\")\n",
    "\n",
    "for a_element in a_elements:\n",
    "    print(a_element['href'])\n",
    "    print(a_element.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Advanced Concept - Lambda Functions\n",
    "\n",
    "# Lambda functions take some explaining. Basically, they're miniature, one-time-use functions that have no name.\n",
    "# They pass in a variable L (or whatever you want to name it), which is defined by BeautifulSoup in this case to be a string.\n",
    "# In this case, we want it to return TRUE if it's a match but FALSE if it's not a match\n",
    "# Imagine a for loop, looping through every 'ul' element tag in the file:\n",
    "\n",
    "# all_the_ul_tags = soup.find_all(\"ul\")\n",
    "# selected_ul_elements = []\n",
    "# for L in all_the_ul_tags:\n",
    "#     if L['id'] and L['id'].endswith(\"_list\"):\n",
    "#         selected_ul_elements.append(L['id'])\n",
    "\n",
    "# This is what the lambda function below is doing. Five lines of code into one line! Much cleaner, once you get the syntax \n",
    "# The BeautifulSoup package understands to make a list of only those TRUE elements\n",
    "# Also, the reason for the strange \"L and\" and the beginning is a fancy way of preventing errors, as I'll explain in the workshop\n",
    "selected_ul_elements = soup.find_all(\"ul\",{'id' : lambda L: L and L.endswith(\"_list\")})\n",
    "\n",
    "for element in selected_ul_elements:\n",
    "    print(\"-----------------------\")\n",
    "    print(element)\n",
    "    print(\"-----------------------\")\n",
    "    print(element.find_all(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another tool you'll find useful is urljoin()\n",
    "# Oftentimes you get what are known as relative links, which need to be joined with the base link to make a full hyperlink\n",
    "# The cool part about urljoin() (vs. string concatenation) is that it doesn't care about making sure the slashes match\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "my_link = \"http://www.google.com/\" + \"/some_random_page.html\"\n",
    "print(my_link)\n",
    "print(urljoin(\"http://www.google.com/\",\"/some_random_page.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     15
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADVANCED Recipe: Download all Forum posts from a specific forum\n",
    "# Note that this is a very complex program! Don't expect to understand it your first time through\n",
    "# With experience, continue to go back to this receipe.\n",
    "# Take pieces of it apart in new cells and play with them\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import unicodecsv as csv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Loop through all the LINKS on a PAGE\n",
    "def go_through_thread_links(all_a_links, base_page):\n",
    "    for forum_link in all_a_links:\n",
    "        print(\"---------------------NEW THREAD---------------------------\")\n",
    "        print(\"Currently on link {0}\".format(forum_link))\n",
    "        response = requests.get(forum_link)\n",
    "        assert response.status_code == 200\n",
    "        civ_text = response.text\n",
    "        soup = BeautifulSoup(civ_text,\"lxml\")\n",
    "        all_posts = soup.find_all(\"li\",{\"id\":lambda L: L and L.startswith(\"post-\")})\n",
    "        page_title = soup.find(\"div\",{\"class\":\"titleBar\"}).find(\"h1\").text\n",
    "        go_through_page_posts(all_posts, base_page, page_title)\n",
    "\n",
    "# Loop through all the POSTS on a LINK\n",
    "def go_through_page_posts(all_posts, base_page, page_title):\n",
    "    for posting in all_posts:\n",
    "        # Get the user!\n",
    "        posting_user = posting.find(\"a\",{ \"itemprop\" : \"name\" }).string\n",
    "        \n",
    "        # Get the datetime\n",
    "        has_span_datetime = posting.find(\"span\",{\"class\" : \"DateTime\"})\n",
    "        if not has_span_datetime:\n",
    "            posting_datetime = posting.find(\"abbr\",{\"class\" : \"DateTime\"}).string\n",
    "        else:\n",
    "            posting_datetime = has_span_datetime.string\n",
    "        \n",
    "        # Get the content and clean it!\n",
    "        posting_text = posting.find(\"blockquote\",{\"class\": \"messageText SelectQuoteContainer ugc baseHtml\"}).text.strip()\n",
    "        posting_text_cleaned = re.sub( '\\s+', ' ', str(posting_text))\n",
    "        #print(posting_text_cleaned)\n",
    "        \n",
    "        # Get the post's ID according to the forum (every post has a unique ID in this forum)\n",
    "        posting_id = posting['id']\n",
    "        \n",
    "        # Prepare in list\n",
    "        my_row = [page_title,posting_user,posting_datetime,posting_id,posting_text_cleaned]\n",
    "        \n",
    "        # Write the Row\n",
    "        my_csv_writer.writerow(my_row)\n",
    "    print(\"Subpage {0} done\".format(page_title)) # Status message\n",
    "\n",
    "# Part One: File setup code - prepare the CSV file. Write the first (title) column\n",
    "output_file = open(\"Output Files/my_forum_results.csv\",\"wb\")\n",
    "my_csv_writer = csv.writer(output_file)\n",
    "my_csv_writer.writerow([\"Page Title\",\"Posting User\",\"Posting Date-Time\",\"Posting ID\",\"Posting Text\"])\n",
    "\n",
    "# Part Two: Prepare the initial soup based on Page 1\n",
    "base_page = \"http://forums.civfanatics.com/\" # A random forum chosen\n",
    "\n",
    "# Separating the \"base page\" from the \"relative link\" is really useful for later\n",
    "initial_civ_link = urljoin(base_page,\"/forums/civ-ideas-suggestions.119/\")\n",
    "\n",
    "response = requests.get(initial_civ_link)\n",
    "assert response.status_code == 200\n",
    "\n",
    "civ_text = response.text\n",
    "\n",
    "# Part Three: Find all other page links from our initial_civ_link page\n",
    "soup = BeautifulSoup(civ_text,\"lxml\")\n",
    "\n",
    "# Get number of pages to go through\n",
    "num_of_pages = int(soup.find(\"a\",{\"class\":\"PageNavNext\"}).find_next_sibling(\"a\").string)\n",
    "\n",
    "# Part Four: Loop through each PAGE.\n",
    "# Note that for speed purposes, I've made the range do only a single page [range(1,2)].\n",
    "# However, if want to get everything, change that line of code below to:\n",
    "# for current_page in range(1,num_of_pages):\n",
    "# But note that it may take up to 2 hours to complete - tens of thousands of forum posts to download!\n",
    "for current_page in range(1,2):\n",
    "    print(\"---------------------NEW PAGE---------------------------\")\n",
    "    current_page_link = initial_civ_link + \"page-\" + str(current_page)\n",
    "    response = requests.get(current_page_link)\n",
    "    assert response.status_code == 200\n",
    "    soup = BeautifulSoup(response.text,\"lxml\")\n",
    "    all_forum_page_tags = soup.find_all(\"li\",{\"id\":lambda L: L and L.startswith(\"thread-\")})\n",
    "    \n",
    "    # Most complex line of code - parsing the links out\n",
    "    all_a_links = [urljoin(base_page, forum_page_tag.find(\"h3\",{\"class\":\"title\"}).a['href']) for forum_page_tag in all_forum_page_tags]\n",
    "    # Finally, call the function that goes through everything inside that page\n",
    "    go_through_thread_links(all_a_links, current_page_link)\n",
    "    \n",
    "print(\"Done!\")\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
